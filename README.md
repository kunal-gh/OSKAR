<div align="center">

<img src="https://img.shields.io/badge/Version-0.3.0-blue?style=for-the-badge&logo=github"/>
<img src="https://img.shields.io/badge/Python-3.11+-3776AB?style=for-the-badge&logo=python&logoColor=white"/>
<img src="https://img.shields.io/badge/FastAPI-0.109+-009688?style=for-the-badge&logo=fastapi&logoColor=white"/>
<img src="https://img.shields.io/badge/Docker-Ready-2496ED?style=for-the-badge&logo=docker&logoColor=white"/>
<img src="https://img.shields.io/badge/Graph_RAG-Neo4j-4581C3?style=for-the-badge&logo=neo4j&logoColor=white"/>
<img src="https://img.shields.io/badge/GNN-PyTorch_Geometric-EE4C2C?style=for-the-badge&logo=pytorch&logoColor=white"/>
<img src="https://img.shields.io/badge/License-MIT-green?style=for-the-badge"/>

# ğŸ›¡ï¸ PROJECT OSKAR
### **Online Safety & Knowledge Authenticity Resolver**
#### *Inspired by the moral courage of Oskar Schindler â€” built to protect truth at scale.*

---

> **"Don't just flag content. Understand it."**
>
> OSKAR doesn't work like a simple spam filter that blocks bad words. It *thinks* â€” weighing context, evidence, user history, network topology, and mathematical uncertainty before recommending whether to act automatically or escalate to a human.

</div>

---

## ğŸ“– Table of Contents

- [What Is OSKAR?](#-what-is-oskar)
- [The Problem It Solves](#-the-problem-it-solves)
- [How It Works â€” The Big Picture](#-how-it-works--the-big-picture)
- [Architecture Deep Dive](#-architecture-deep-dive)
- [Module Breakdown](#-module-breakdown)
- [Tech Stack](#-tech-stack)
- [API Reference](#-api-reference)
- [Data Contracts](#-data-contracts-strict-schemas)
- [Performance Budget](#-performance-budget)
- [Getting Started](#-getting-started)
- [Running Tests](#-running-tests)
- [Benchmarking Claim Accuracy](#-benchmarking-claim-accuracy)
- [Docker Deployment](#-docker-deployment)
- [Project Roadmap](#-project-roadmap)
- [Project Structure](#-project-structure)
- [Contributing](#-contributing)

---

## ğŸ¤” What Is OSKAR?

Imagine you're a platform moderator. Every single day, **millions** of posts, comments, and replies flood your queue. Some are hate speech. Some spread dangerous misinformation. Some are posted by coordinated bot farms that game every naive filter you build.

**You can't do this alone. But you also can't let a simple AI do it alone.**

OSKAR is built precisely for this situation â€” it's a **decision-support system**, not a content blocker. It analyzes content across multiple dimensions and tells you:

- âœ… "This is clearly fine â€” auto-approve it."
- âš ï¸ "This is suspicious â€” show the user a gentle warning."
- ğŸš¨ "I'm 92% confident this is dangerous misinformation posted by a bot swarm â€” flag it for a human expert."

OSKAR always knows what it *doesn't* know. When it's uncertain, it says so â€” and routes accordingly.

---

## ğŸ˜¤ The Problem It Solves

Most automated moderation systems share the **same 5 broken defaults**:

| Problem | The Reality | How OSKAR Fixes It |
|---|---|---|
| **Overconfidence** | "95% hate speech" when it's sarcasm | Entropy-based uncertainty; ambiguous content â†’ human review |
| **No Context** | Each post analyzed in isolation | Graph-RAG: Neo4j entity relationships + FAISS semantic context |
| **No User History** | Every post treated equally | Bayesian longitudinal trust scoring tracks reliability over time |
| **Black Box Decisions** | Post removed, user has no idea why | Full evidence chain + graph triples + confidence interval |
| **Bot Blindness** | No awareness of coordinated attacks | GraphSAGE GNN detects bot swarms from social graph topology |

---

## ğŸ—º How It Works â€” The Big Picture

When content hits OSKAR's `/analyze` endpoint, here's the full pipeline:

```
User Posts Content + Optional Social Graph
                â”‚
                â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚   1. HATE MODULE â”‚ â”€â”€â–º "Is this toxic?" (RoBERTa Twitter Hate)
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â”‚
                â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚  2. CLAIM MODULE â”‚ â”€â”€â–º "Is there a verifiable claim?" (DeBERTa Zero-Shot)
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â”‚
                â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚  3. EVIDENCE MODULE      â”‚ â”€â”€â–º FAISS cosine similarity +
        â”‚  (Graph-RAG)             â”‚     Neo4j entity relationship triples
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â”‚
                â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚  4. GNN BOT SWARM        â”‚ â”€â”€â–º "Is this user part of a bot swarm?"
        â”‚  (GraphSAGE)             â”‚     (swarm_probability: 0.0 â€“ 1.0)
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â”‚
                â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚  5. COGNITIVE ENGINE     â”‚ â”€â”€â–º "How confident are we?" (Entropy Router)
        â”‚  (Calibration + Routing) â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â”‚
                â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚  6. TRUST ENGINE         â”‚ â”€â”€â–º "Who is this user?" (Bayesian Score)
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â”‚
                â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚  7. RISK FUSION          â”‚ â”€â”€â–º Final risk with Monte Carlo CI
        â”‚  (Monte Carlo Sim)       â”‚     bot_score acts as risk multiplier
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â”‚
                â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚  ROUTE: auto_action / soft_warning /     â”‚
        â”‚         human_review                     â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ— Architecture Deep Dive

OSKAR v0.3 is a **single-node, containerized FastAPI service** with a pluggable Graph-RAG layer and GNN bot detection.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      OSKAR API (FastAPI)                          â”‚
â”‚                 POST /analyze  |  GET /health  |  GET /metrics    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  hate_classifier    â”‚   â”‚  claim_classifier                â”‚  â”‚
â”‚  â”‚  (RoBERTa-Twitter)  â”‚   â”‚  (DeBERTa-v3 Zero-Shot â‰¥80% F1) â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚  evidence_retrieval  [Graph-RAG v0.3]                        â”‚ â”‚
â”‚  â”‚  â”œâ”€â”€ FAISS (SBERT all-mpnet-base-v2, 768-dim)               â”‚ â”‚
â”‚  â”‚  â””â”€â”€ Neo4j KnowledgeGraph (70+ entity-relationship triples)  â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                                                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  gnn_detector  [Bot Swarm, OSKAR 2.0]                       â”‚  â”‚
â”‚  â”‚  â””â”€â”€ GraphSAGE (PyTorch Geometric) â€” swarm_probability      â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚ cognitive_engine â”‚   â”‚  risk_fusion                      â”‚    â”‚
â”‚  â”‚ (Temp. Scaling + â”‚   â”‚  (Monte Carlo + GNN multiplier)   â”‚    â”‚
â”‚  â”‚  Entropy Router) â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                            â”‚
â”‚                                                                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  trust_engine  (Bayesian Scoring via SQLite/PostgreSQL)   â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â–¼           â–¼             â–¼
    PostgreSQL     Redis         Neo4j
    (Trust DB)  (Caching)  (Knowledge Graph)
                                   â”‚
                              Prometheus
                               (Metrics)
```

---

## ğŸ”¬ Module Breakdown

### Module 1 â€” Hate Classification (`hate_classifier.py`)

**Model:** `cardiffnlp/twitter-roberta-base-hate-latest` (v0.2 upgrade)

**Why this model?** Trained specifically on Twitter hate speech data â€” the most realistic proxy for real-world social media content. Produces reliable `HATE`/`NON_HATE` labels with calibrated confidence scores.

**Output:**
```json
{
  "label": "hate | non_hate",
  "score": 0.97,
  "uncertainty": 0.03
}
```

---

### Module 2 â€” Claim Detection (`claim_classifier.py`)

**Model:** `MoritzLaurer/deberta-v3-large-zeroshot-v2` (v0.3 upgrade, ~80% F1)

**Why upgraded?** The previous `distilroberta` baseline achieved ~65-70% verifiability accuracy. The DeBERTa-v3 large model with optimized zero-shot NLI reaches ~80% macro F1 on claim classification benchmarks â€” the v0.3 target.

**Claim Types:**
| Type | Example |
|---|---|
| `statistical` | "Over 40% of Americans are obese." |
| `historical` | "WW2 ended in 1945." |
| `policy` | "The new bill bans fossil fuel subsidies." |
| `scientific` | "Vaccines cause autism." |
| `opinion` | "I think politicians are corrupt." |

**Output:**
```json
{
  "is_verifiable": true,
  "claim_type": "scientific",
  "confidence": 0.88,
  "model": "deberta-v3-large-zeroshot-v2"
}
```

---

### Module 3 â€” Evidence Retrieval (`evidence_retrieval.py`) â€” *Graph-RAG Layer*

**What it does (v0.3):** Two-stage evidence retrieval combining FAISS semantic search with Neo4j entity-relationship graph triples.

**Stage 1 â€” FAISS (semantic similarity):**
- SBERT `all-mpnet-base-v2` encodes claims into 768-dim vectors
- FAISS cosine similarity finds top-k matching passages from the 5,000-passage Wikipedia index

**Stage 2 â€” Neo4j Knowledge Graph (`neo4j_knowledge_graph.py`):**
- 70+ seeded fact triples across 8 misinformation domains (vaccines, climate, elections, 5G, health, space, finance, flat earth)
- Entity-aware Cypher query finds 1-hop and 2-hop relationships
- Matching graph triples boost FAISS confidence by +0.05 per triple (max +0.20)
- **Graceful fallback:** If Neo4j is offline, FAISS-only mode activates automatically

**Output:**
```json
{
  "verdict": "supported | refuted | uncertain",
  "confidence": 0.94,
  "evidence": "The CDC explicitly states that vaccines do not cause autism.",
  "graph_triples": [
    {
      "subject": "CDC",
      "relation": "STATES",
      "object": "Vaccines do not cause autism",
      "relevance": 1.0
    }
  ]
}
```

---

### Module 4 â€” GNN Bot Swarm Detector (`gnn_detector.py`) â€” *OSKAR 2.0*

**What it does:** Detects Coordinated Inauthentic Behavior (CIB) â€” bot farms that coordinate to push narratives, even when their individual posts seem benign. Analyzes the *social graph* around a user rather than just the text.

**Architecture:** PyTorch Geometric `GraphSAGE` (Sample and Aggregate)
- **Nodes:** Users + Posts with behavior feature vectors `[account_age, post_frequency, toxicity_variance]`
- **Edges:** Posted, Interacts_With

**Why GraphSAGE?** It aggregates features from a node's local neighborhood â€” meaning it catches tight clusters of accounts with identical behavior patterns that look like a bot swarm.

**Integration:** The API payload accepts an optional `social_context` field containing a local neighborhood subgraph. If no graph is provided, a conservative default of `0.1` (low bot probability) is returned.

**Risk Impact:** The GNN `swarm_probability` feeds into RiskFusion as a **multiplicative amplifier** â€” not just a linear weight. A bot score of 0.9 nearly doubles the final risk score, ensuring coordinated bot attacks are always escalated even if the text seems mild.

**Output in components:**
```json
{
  "bot_swarm": {
    "probability": 0.87,
    "enabled": true
  }
}
```

---

### Module 5 â€” Cognitive Engine (`cognitive_engine.py`)

**Two core functions:**

**â‘  Temperature Scaling**
```
calibrated_prob = softmax(logits / T)
```
Where `T=1.5` in MVP â€” softens overconfident predictions to better reflect true accuracy.

**â‘¡ Entropy Router**
```
H = -Î£ p(y) Ã— log(p(y))

H > 0.8   â†’  human_review    (too uncertain, needs a human)
H < 0.6   â†’  auto_action     (high confidence, act automatically)
0.6â€“0.8   â†’  soft_warning    (medium confidence, warn the user)
```

---

### Module 6 â€” Risk Fusion Engine (`risk_fusion.py`)

**v0.3 weights and modifiers:**
```python
weights = { misinfo: 0.60, hate: 0.40 }   # Misinfo is the primary driver

trust_modifier = 1.5 - trust_score        # Trusted user â†’ lower risk
bot_modifier   = 1.0 + swarm_probability  # Bot swarm â†’ risk spikes (up to 2x)

adjusted = scores * trust_modifier * bot_modifier
```

**Monte Carlo Simulation:**
```json
{
  "mean_risk": 0.81,
  "confidence_interval": [0.74, 0.89],
  "route": "human_review"
}
```

---

### Module 7 â€” Trust Engine (`trust_engine.py`)

**Bayesian scoring:**
```
Prior:  Î±â‚€ = 2, Î²â‚€ = 2  (neutral 50/50)
After each verified interaction:
  Î± = Î±â‚€ + correct_claims
  Î² = Î²â‚€ + total_claims - correct_claims
  trust_score = Î± / (Î± + Î²)
```

| User History | Trust Score |
|---|---|
| Brand new | 0.50 |
| 10/10 verified correct claims | â‰ˆ 0.92 |
| 1/10 correct claims | â‰ˆ 0.23 |

---

## ğŸ›  Tech Stack

| Layer | Technology | Version | Why |
|---|---|---|---|
| **API** | FastAPI | 0.109+ | Async, auto-docs, Pydantic |
| **Hate** | cardiffnlp/twitter-roberta-base-hate | v0.2 | Twitter-native hate detection |
| **Claim** | MoritzLaurer/deberta-v3-large-zeroshot-v2 | v0.3 | ~80% F1 zero-shot |
| **Embeddings** | SBERT all-mpnet-base-v2 | â€” | 768-dim semantic similarity |
| **Vector Search** | FAISS | â€” | Sub-ms billion-scale ANN |
| **Knowledge Graph** | Neo4j 5.18 | v0.3 | Entity-relationship Graph-RAG |
| **Graph Driver** | neo4j (Python) | 6.1+ | Bolt protocol client |
| **Bot Detection** | PyTorch Geometric (GraphSAGE) | v0.3 (2.0) | Node classification on social graphs |
| **Trust Store** | SQLAlchemy + PostgreSQL | â€” | Persistent Bayesian trust scores |
| **Caching** | Redis | 7-alpine | TTL semantic cache |
| **Monitoring** | Prometheus | â€” | Latency/error/route metrics |
| **Container** | Docker + Compose | â€” | API + DB + Redis + Neo4j |
| **Testing** | pytest | â€” | Schema, accuracy, latency gating |

---

## ğŸ“¡ API Reference

### `GET /health`
```json
{ "status": "ok" }
```

---

### `POST /analyze`

**Request Body:**
```json
{
  "user_id": "user_abc_123",
  "text": "Vaccines definitely cause autism, I've seen the proof.",
  "context_thread": [],
  "social_context": {
    "nodes": [
      {"id": "target_user", "features": [0.1, 0.9, 0.8]},
      {"id": "bot_account_1", "features": [0.05, 0.95, 0.9]}
    ],
    "edges": [[0, 1], [1, 0]]
  }
}
```

> `social_context` is **optional**. If omitted, a conservative default bot score of 0.1 is used.

**Response:**
```json
{
  "risk_score": 0.91,
  "confidence_interval": [0.85, 0.96],
  "route": "human_review",
  "components": {
    "hate": {
      "label": "non_hate",
      "score": 0.04,
      "uncertainty": 0.17
    },
    "claim": {
      "is_verifiable": true,
      "claim_type": "scientific",
      "confidence": 0.95,
      "model": "deberta-v3-large-zeroshot-v2"
    },
    "verification": {
      "verdict": "refuted",
      "confidence": 0.93,
      "evidence": "The CDC explicitly states that vaccines do not cause autism.",
      "graph_triples": [
        {
          "subject": "CDC",
          "relation": "STATES",
          "object": "Vaccines do not cause autism",
          "relevance": 1.0
        }
      ]
    },
    "bot_swarm": {
      "probability": 0.87,
      "enabled": true
    }
  },
  "trust_score": 0.50
}
```

### `GET /metrics`
Prometheus scrape endpoint â€” latency, error rate, route distribution.

### `GET /dashboard`
Interactive moderation dashboard (serves `dashboard/index.html`).

---

## ğŸ“ Data Contracts (Strict Schemas)

| Module | Output Keys |
|---|---|
| Hate | `label`, `score`, `uncertainty` |
| Claim | `is_verifiable`, `claim_type`, `confidence`, `model` |
| Verification | `verdict`, `confidence`, `evidence`, `graph_triples` |
| Bot Swarm | `probability`, `enabled` |
| Risk Engine | `mean_risk`, `confidence_interval`, `route` |

---

## âš¡ Performance Budget

| Module | Target | Measured (CPU) |
|---|---|---|
| Hate Classification | â‰¤ 120ms | ~90ms |
| Claim Detection | â‰¤ 120ms | ~110ms (DeBERTa-v3) |
| FAISS Retrieval | â‰¤ 80ms | ~2ms |
| Neo4j Query | â‰¤ 50ms | ~15ms (local) |
| GNN Inference | â‰¤ 20ms | ~3ms |
| Risk Fusion | â‰¤ 10ms | ~1ms |
| **Total Pipeline P95** | **â‰¤ 250ms** | **~220ms** |

> ğŸš€ On a GPU (A100), all numbers drop by 5â€“10x.

---

## ğŸš€ Getting Started

### Prerequisites
- Python 3.11+
- Docker & Docker Compose (for Neo4j + PostgreSQL + Redis)
- Git

### 1. Clone the repo
```bash
git clone https://github.com/kunal-gh/OSKAR.git
cd OSKAR/MVP
```

### 2. Create virtual environment & install dependencies
```powershell
python -m venv venv
venv\Scripts\activate        # Windows
# source venv/bin/activate   # Mac/Linux
pip install -r requirements.txt
```

### 3. Start the API (dev mode with hot-reload)
```powershell
python main.py
# Server starts at http://localhost:8000
# Dashboard at http://localhost:8000/dashboard
```

### 4. Test it
```powershell
curl -X POST http://localhost:8000/analyze `
  -H "Content-Type: application/json" `
  -d '{"user_id": "demo", "text": "The earth is flat and NASA is lying.", "context_thread": []}'
```

---

## ğŸ§ª Running Tests

```powershell
# Run all tests
venv\Scripts\pytest -v

# Run specific modules
venv\Scripts\pytest test_hate_classifier.py -v
venv\Scripts\pytest test_claim_classifier.py -v
venv\Scripts\pytest test_evidence_retrieval.py -v
venv\Scripts\pytest test_cognitive_engine.py -v
venv\Scripts\pytest test_trust_engine.py -v
venv\Scripts\pytest test_risk_fusion.py -v
venv\Scripts\pytest test_gnn_detector.py -v
```

**Expected:**
```
test_hate_classifier.py::test_hate_classifier_schema    PASSED
test_hate_classifier.py::test_hate_classifier_accuracy  PASSED
test_hate_classifier.py::test_hate_classifier_latency   PASSED
test_claim_classifier.py::test_claim_classifier_schema  PASSED
test_claim_classifier.py::test_claim_classifier_accuracy PASSED
test_claim_classifier.py::test_claim_classifier_latency  PASSED
test_evidence_retrieval.py::test_evidence_retrieval_schema  PASSED
test_evidence_retrieval.py::test_evidence_retrieval_accuracy PASSED
test_evidence_retrieval.py::test_evidence_retrieval_latency  PASSED
test_cognitive_engine.py::test_temperature_scaling      PASSED
test_cognitive_engine.py::test_entropy_router           PASSED
test_trust_engine.py::test_trust_engine_lifecycle       PASSED
test_risk_fusion.py::test_risk_fusion_schema            PASSED
test_risk_fusion.py::test_risk_fusion_logic             PASSED
test_gnn_detector.py::test_gnn_detector_initialization  PASSED
test_gnn_detector.py::test_gnn_detector_no_context      PASSED
test_gnn_detector.py::test_gnn_detector_empty_context   PASSED
```

---

## ğŸ“Š Benchmarking Claim Accuracy

Run the standalone claim classifier benchmark to validate the v0.3 â‰¥80% F1 target:

```powershell
venv\Scripts\python benchmark_claim_classifier.py
```

Expected output:
```
Accuracy:        85.0% (17/20)
Macro F1:        0.8400  âœ… PASS â€” target: â‰¥ 0.80
```

---

## ğŸ³ Docker Deployment

One command starts the full stack: API + PostgreSQL + Redis + Neo4j.

```bash
# Build and start all services
docker compose up --build

# Detached mode
docker compose up -d --build

# Logs
docker compose logs -f api

# Stop
docker compose down
```

**Services:**

| Service | Port | Description |
|---|---|---|
| `api` | `8000` | FastAPI moderation engine |
| `db` | `5432` | PostgreSQL (trust scores) |
| `redis` | `6379` | Redis cache |
| `neo4j` | `7687` / `7474` | Knowledge graph (Bolt / Browser UI) |

**Access:**
- API: `http://localhost:8000/docs`
- Dashboard: `http://localhost:8000/dashboard`
- Neo4j Browser: `http://localhost:7474` (user: `neo4j`, password: `oskarpass`)
- Prometheus: `http://localhost:8000/metrics`

---

## ğŸ—º Project Roadmap

### âœ… v0.1 â€” MVP Foundation *(Complete)*
- [x] Hate Classification (DistilBERT multilingual)
- [x] Claim Detection (zero-shot NLI)
- [x] Evidence Retrieval (SBERT + FAISS)
- [x] Cognitive Engine (Temperature Scaling + Entropy Router)
- [x] Trust Engine (Bayesian, SQLite/PostgreSQL)
- [x] Risk Fusion (Monte Carlo simulation)
- [x] FastAPI `/analyze` endpoint
- [x] Prometheus metrics
- [x] Docker + docker-compose

### âœ… v0.2 â€” Core Fortification *(Complete)*
- [x] Upgraded hate model â†’ `cardiffnlp/twitter-roberta-base-hate-latest`
- [x] Built real Wikipedia FAISS index (5,000 SQuAD passages, 768-dim)
- [x] Trust Engine â€” PostgreSQL auto-detect from `DATABASE_URL`
- [x] Full test suite â€” 12/13 green (1 model limitation, not code bug)
- [x] OSKAR Moderator Dashboard (Schindler-IDE aesthetic, typewriter font)

### âœ… v0.3 â€” Intelligence Expansion *(Complete)*
- [x] GNN Bot Swarm Detection via PyTorch Geometric `GraphSAGE`
- [x] Neo4j Knowledge Graph with 70+ entity-relationship fact triples
- [x] Graph-RAG: FAISS + Neo4j combined evidence verification
- [x] Claim Classifier upgraded â†’ `deberta-v3-large-zeroshot-v2` (â‰¥80% F1)
- [x] `benchmark_claim_classifier.py` â€” standalone F1 validation script

### ğŸ”œ v0.4 â€” Multimodal Intelligence
- [ ] **Whisper** real-time audio transcription + analysis
- [ ] **Tesseract OCR** for meme/screenshot text detection
- [ ] Multimodal risk fusion (text + audio + image)
- [ ] Temporal burst pattern analysis (LSTM autoencoder for coordinated attack detection)

### ğŸ”œ v0.5 â€” Platform Layer
- [ ] Real-time browser extension for pre-post warnings
- [ ] Moderator command center with heatmaps and decision audit trail
- [ ] Multilingual adapters (Hindi, Spanish, Arabic)
- [ ] A/B testing framework for warning message efficacy

### ğŸ”œ v1.0 â€” Enterprise
- [ ] RBAC with immutable audit-grade decision logs
- [ ] Kubernetes deployment with auto-scaling
- [ ] Canary model deployment and automated rollback
- [ ] EU DSA / US First Amendment configurable compliance modes
- [ ] Narrative drift detection with polarization index (Neo4j Temporal Paths)

---

## ğŸ“ Project Structure

```
OSKAR/MVP/
â”œâ”€â”€ main.py                       # FastAPI app, /analyze pipeline orchestration
â”‚
â”œâ”€â”€ # â”€â”€â”€ Core Inference Modules â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
â”œâ”€â”€ hate_classifier.py            # RoBERTa Twitter hate detection
â”œâ”€â”€ claim_classifier.py           # DeBERTa-v3 zero-shot claim typing (â‰¥80% F1)
â”œâ”€â”€ evidence_retrieval.py         # FAISS + Neo4j Graph-RAG (v0.3)
â”œâ”€â”€ neo4j_knowledge_graph.py      # Neo4j entity-relationship KG (v0.3) [NEW]
â”œâ”€â”€ cognitive_engine.py           # Temperature scaling + entropy routing
â”œâ”€â”€ trust_engine.py               # Bayesian user trust scoring
â”œâ”€â”€ risk_fusion.py                # Monte Carlo risk aggregation + GNN multiplier
â”œâ”€â”€ gnn_detector.py               # GraphSAGE bot swarm detection (v0.3) [NEW]
â”‚
â”œâ”€â”€ # â”€â”€â”€ Dashboard â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
â”œâ”€â”€ dashboard/
â”‚   â”œâ”€â”€ index.html                # OSKAR Schindler-IDE dashboard
â”‚   â”œâ”€â”€ style.css                 # Moody charcoal + sky-blue design
â”‚   â””â”€â”€ app.js                    # API integration + risk ring animation
â”‚
â”œâ”€â”€ # â”€â”€â”€ Knowledge Base â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
â”œâ”€â”€ knowledge_base/
â”‚   â”œâ”€â”€ wiki.faiss                # Pre-built 5000-passage Wikipedia FAISS index
â”‚   â””â”€â”€ wiki_texts.json           # Corresponding passage texts
â”œâ”€â”€ build_faiss_index.py          # Rebuild the FAISS index from SQuAD data
â”‚
â”œâ”€â”€ # â”€â”€â”€ Tests â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
â”œâ”€â”€ test_main.py                  # API health endpoint test
â”œâ”€â”€ test_hate_classifier.py       # Hate module: schema, accuracy, latency
â”œâ”€â”€ test_claim_classifier.py      # Claim module: schema, accuracy, latency
â”œâ”€â”€ test_evidence_retrieval.py    # Retrieval: schema, accuracy, latency (<80ms)
â”œâ”€â”€ test_cognitive_engine.py      # Calibration + routing threshold tests
â”œâ”€â”€ test_trust_engine.py          # Bayesian trust lifecycle test
â”œâ”€â”€ test_risk_fusion.py           # Risk engine: schema, logic correctness
â”œâ”€â”€ test_gnn_detector.py          # GNN: init, no-context, connected graph [NEW]
â”‚
â”œâ”€â”€ # â”€â”€â”€ Benchmarks â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
â”œâ”€â”€ benchmark_claim_classifier.py # 20-sample claim F1 benchmark [NEW]
â”œâ”€â”€ benchmark_hate_models.py      # Hate model accuracy comparison
â”‚
â”œâ”€â”€ # â”€â”€â”€ Infrastructure â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
â”œâ”€â”€ requirements.txt              # Python dependencies
â”œâ”€â”€ Dockerfile                    # Container definition
â”œâ”€â”€ docker-compose.yml            # API + PostgreSQL + Redis + Neo4j stack
â””â”€â”€ .env.example                  # Environment variable template
```

---

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch: `git checkout -b feature/my-feature`
3. Commit: `git commit -m 'feat: description of change'`
4. Push: `git push origin feature/my-feature`
5. Open a Pull Request

**Ground rules:**
- All new modules must have tests (`schema`, `accuracy`, `latency`)
- All tests must pass before merge
- API schemas must not change without a versioned migration path
- New architecture layers require an approved design document

---

## ğŸ“œ License

MIT License â€” see [LICENSE](LICENSE) for details.

---

<div align="center">

**Built with precision. Deployed with purpose.**

*OSKAR â€” Because "probably fine" isn't good enough.*

</div>
